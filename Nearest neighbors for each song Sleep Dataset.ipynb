{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    iterates over the million playlist dataset and outputs info\n",
    "    about what is in there.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        python stats.py path-to-mpd-data\n",
    "\"\"\"\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "total_playlists = 0\n",
    "total_tracks = 0\n",
    "tracks = set()\n",
    "artists = set()\n",
    "albums = set()\n",
    "titles = set()\n",
    "total_descriptions = 0\n",
    "ntitles = set()\n",
    "title_histogram = collections.Counter()\n",
    "artist_histogram = collections.Counter()\n",
    "track_histogram = collections.Counter()\n",
    "last_modified_histogram = collections.Counter()\n",
    "num_edits_histogram = collections.Counter()\n",
    "playlist_length_histogram = collections.Counter()\n",
    "num_followers_histogram = collections.Counter()\n",
    "\n",
    "\n",
    "track_dict = {}\n",
    "\n",
    "playlist_dict = {}\n",
    "\n",
    "haha = len(playlist_dict)\n",
    "\n",
    "quick = False\n",
    "max_files_for_quick_processing = 5\n",
    "\n",
    "def process_mpd(path):\n",
    "    count = 0\n",
    "    fullpath = file_path_1\n",
    "    f = open(fullpath)\n",
    "    js = f.read()\n",
    "    f.close()\n",
    "    mpd_slice = json.loads(js)\n",
    "    #process_info(mpd_slice['info'])\n",
    "    for playlist in mpd_slice['playlists']:\n",
    "        process_playlist(playlist)\n",
    "\n",
    "\n",
    "    #if quick and count > max_files_for_quick_processing:\n",
    "     #   break\n",
    "\n",
    "    #show_summary()\n",
    "\n",
    "\n",
    "def show_summary():\n",
    "    print\n",
    "    print \"number of playlists\", total_playlists\n",
    "    print \"number of tracks\", total_tracks\n",
    "    print \"number of unique tracks\", len(tracks)\n",
    "    print \"number of unique albums\", len(albums)\n",
    "    print \"number of unique artists\", len(artists)\n",
    "    print \"number of unique titles\", len(titles)\n",
    "    print \"number of playlists with descriptions\", total_descriptions\n",
    "    print \"number of unique normalized titles\", len(ntitles)\n",
    "    print \"avg playlist length\", float(total_tracks) / total_playlists\n",
    "    print\n",
    "    print \"top playlist titles\"\n",
    "    for title, count in title_histogram.most_common(20):\n",
    "        print \"%7d %s\" % (count, title)\n",
    "\n",
    "    print\n",
    "    print \"top tracks\"\n",
    "    for track, count in track_histogram.most_common(20):\n",
    "        print \"%7d %s\" % (count, track)\n",
    "\n",
    "    print\n",
    "    print \"top artists\"\n",
    "    for artist, count in artist_histogram.most_common(20):\n",
    "        print \"%7d %s\" % (count, artist)\n",
    "\n",
    "    print\n",
    "    print \"numedits histogram\"\n",
    "    for num_edits, count in num_edits_histogram.most_common(20):\n",
    "        print \"%7d %d\" % (count, num_edits)\n",
    "\n",
    "    print\n",
    "    print \"last modified histogram\"\n",
    "    for ts, count in last_modified_histogram.most_common(20):\n",
    "        print \"%7d %s\" % (count, to_date(ts))\n",
    "\n",
    "    print\n",
    "    print \"playlist length histogram\"\n",
    "    for length, count in playlist_length_histogram.most_common(20):\n",
    "        print \"%7d %d\" % (count, length)\n",
    "\n",
    "    print\n",
    "    print \"num followers histogram\"\n",
    "    for followers, count in num_followers_histogram.most_common(20):\n",
    "        print \"%7d %d\" % (count, followers)\n",
    "\n",
    "\n",
    "def normalize_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "\n",
    "def to_date(epoch):\n",
    "    return datetime.datetime.fromtimestamp(epoch).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def process_playlist(playlist):\n",
    "    global total_playlists, total_tracks, total_descriptions\n",
    "\n",
    "    total_playlists += 1\n",
    "    # print playlist['playlist_id'], playlist['name']\n",
    "\n",
    "    if 'description' in playlist:\n",
    "        total_descriptions += 1\n",
    "\n",
    "    titles.add(playlist['name'])\n",
    "    nname = normalize_name(playlist['name'])\n",
    "    #haha = len(playlist_dict) + 1\n",
    "    playlist_dict[total_playlists - 1] = []\n",
    "    \n",
    "    ntitles.add(nname)\n",
    "    title_histogram[nname] += 1\n",
    "\n",
    "    playlist_length_histogram[playlist['num_tracks']] += 1\n",
    "    last_modified_histogram[playlist['modified_at']] += 1\n",
    "    num_edits_histogram[playlist['num_edits']] += 1\n",
    "    num_followers_histogram[playlist['num_followers']] += 1\n",
    "\n",
    "    for track in playlist['tracks']:\n",
    "        total_tracks += 1\n",
    "        albums.add(track['album_uri'])\n",
    "        tracks.add(track['track_uri'])\n",
    "        artists.add(track['artist_uri'])\n",
    "        \n",
    "        \n",
    "        track_name = normalize_name(track['track_uri'])\n",
    "        if track_name not in track_dict:\n",
    "            track_dict[track_name] = 1\n",
    "        else:\n",
    "            track_dict[track_name] += 1\n",
    "            \n",
    "        if track_name not in playlist_dict[total_playlists - 1]:\n",
    "            playlist_dict[total_playlists - 1].append(track_name)\n",
    "\n",
    "        full_name = track['track_name'] + \" by \" + track['artist_name']\n",
    "        artist_histogram[track['artist_name']] += 1\n",
    "        track_histogram[full_name] += 1\n",
    "\n",
    "\n",
    "def process_info(_):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = 'sleep.json'\n",
    "list_of_files = []\n",
    "list_of_files.append(file_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(list_of_files)):\n",
    "    process_mpd(list_of_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of playlists 3639\n",
      "number of tracks 170613\n",
      "number of unique tracks 59192\n",
      "number of unique albums 28315\n",
      "number of unique artists 13822\n",
      "number of unique titles 53\n",
      "number of playlists with descriptions 44\n",
      "number of unique normalized titles 6\n",
      "avg playlist length 46.8845836768\n",
      "\n",
      "top playlist titles\n",
      "   3513 sleep\n",
      "     39 sleepðŸ˜´\n",
      "     36 sleepðŸ’¤\n",
      "     21 sleep ðŸ’¤\n",
      "     16 sleep ðŸ˜´\n",
      "     14 s l e e p\n",
      "\n",
      "top tracks\n",
      "    309 All I Want by Kodaline\n",
      "    295 Photograph by Ed Sheeran\n",
      "    283 Let It Go by James Bay\n",
      "    282 Skinny Love by Birdy\n",
      "    275 Skinny Love by Bon Iver\n",
      "    266 Fix You by Coldplay\n",
      "    266 Say Something by A Great Big World\n",
      "    262 The Scientist by Coldplay\n",
      "    260 Thinking Out Loud by Ed Sheeran\n",
      "    256 Chasing Cars by Snow Patrol\n",
      "    241 To Build A Home by The Cinematic Orchestra\n",
      "    239 Let Her Go by Passenger\n",
      "    233 Holocene by Bon Iver\n",
      "    223 Lost Boy by Ruth B.\n",
      "    218 I Won't Give Up by Jason Mraz\n",
      "    218 The A Team by Ed Sheeran\n",
      "    217 A Thousand Years by Christina Perri\n",
      "    216 Say You Won't Let Go by James Arthur\n",
      "    212 How to Save a Life by The Fray\n",
      "    205 i hate u, i love u (feat. olivia o'brien) by gnash\n",
      "\n",
      "top artists\n",
      "   3466 Ed Sheeran\n",
      "   1844 John Mayer\n",
      "   1787 Coldplay\n",
      "   1689 Drake\n",
      "   1645 Bon Iver\n",
      "   1393 One Direction\n",
      "   1250 Sam Smith\n",
      "   1035 Lana Del Rey\n",
      "   1023 Twenty One Pilots\n",
      "    996 The Weeknd\n",
      "    955 Jack Johnson\n",
      "    939 Adele\n",
      "    931 Justin Bieber\n",
      "    873 Shawn Mendes\n",
      "    767 Frank Ocean\n",
      "    761 The Fray\n",
      "    751 Birdy\n",
      "    732 The Lumineers\n",
      "    697 Hozier\n",
      "    677 The 1975\n",
      "\n",
      "numedits histogram\n",
      "    360 2\n",
      "    350 3\n",
      "    321 4\n",
      "    268 5\n",
      "    227 6\n",
      "    169 7\n",
      "    169 8\n",
      "    155 9\n",
      "    128 10\n",
      "    105 11\n",
      "     95 12\n",
      "     92 15\n",
      "     86 13\n",
      "     74 14\n",
      "     70 18\n",
      "     65 16\n",
      "     61 19\n",
      "     59 17\n",
      "     48 21\n",
      "     44 22\n",
      "\n",
      "last modified histogram\n",
      "     39 2017-10-29\n",
      "     34 2017-10-28\n",
      "     33 2017-10-30\n",
      "     32 2017-10-25\n",
      "     29 2017-10-24\n",
      "     27 2017-10-22\n",
      "     27 2017-10-26\n",
      "     24 2017-10-27\n",
      "     23 2017-10-19\n",
      "     23 2017-10-18\n",
      "     23 2017-10-17\n",
      "     22 2017-10-08\n",
      "     22 2017-10-23\n",
      "     19 2017-10-05\n",
      "     19 2017-10-16\n",
      "     18 2017-10-15\n",
      "     16 2017-10-12\n",
      "     16 2017-10-11\n",
      "     15 2017-10-21\n",
      "     14 2017-09-13\n",
      "\n",
      "playlist length histogram\n",
      "     84 14\n",
      "     83 20\n",
      "     81 16\n",
      "     79 12\n",
      "     78 11\n",
      "     77 10\n",
      "     77 13\n",
      "     75 15\n",
      "     75 17\n",
      "     75 25\n",
      "     73 22\n",
      "     71 23\n",
      "     62 8\n",
      "     62 21\n",
      "     61 18\n",
      "     60 24\n",
      "     60 28\n",
      "     58 27\n",
      "     57 6\n",
      "     57 9\n",
      "\n",
      "num followers histogram\n",
      "   2852 1\n",
      "    518 2\n",
      "    138 3\n",
      "     56 4\n",
      "     24 5\n",
      "     11 7\n",
      "     10 6\n",
      "      9 8\n",
      "      6 9\n",
      "      2 10\n",
      "      2 11\n",
      "      2 17\n",
      "      1 13\n",
      "      1 14\n",
      "      1 15\n",
      "      1 16\n",
      "      1 40\n",
      "      1 171\n",
      "      1 23\n",
      "      1 39\n"
     ]
    }
   ],
   "source": [
    "show_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59192\n",
      "3639\n"
     ]
    }
   ],
   "source": [
    "print len(track_dict)\n",
    "print len(playlist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = sorted(track_dict, key=track_dict.__getitem__, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = new_array[0:25000]\n",
    "context = new_array[0:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, u'spotify track 5jua3wlm0kn7ihfbehv0i6', 309)\n",
      "\n",
      "(2, u'spotify track 6fxvffatuwjgek5h9qyrjy', 295)\n",
      "\n",
      "(3, u'spotify track 13hvjjwuzfawilh2qujksp', 283)\n",
      "\n",
      "(4, u'spotify track 4rl77hmwuq35nynplxbpih', 282)\n",
      "\n",
      "(5, u'spotify track 3zmv9ezgoteni5qnx0kpeo', 275)\n",
      "\n",
      "(6, u'spotify track 7lvhvu3twfcxj5aipfew4q', 266)\n",
      "\n",
      "(7, u'spotify track 75jfxki2rxiu7l9vxzmkle', 262)\n",
      "\n",
      "(8, u'spotify track 1slwb6doykblwal1pgtnng', 260)\n",
      "\n",
      "(9, u'spotify track 11bd1jtsjligkgzg2134dz', 256)\n",
      "\n",
      "(10, u'spotify track 54kfqb6n4pn926iuuyzgzk', 241)\n",
      "\n",
      "(11, u'spotify track 2jyjhrf6dvbmpu5zxagn2h', 239)\n",
      "\n",
      "(12, u'spotify track 1xqnztr0er8y5fgo17ux1r', 233)\n",
      "\n",
      "(13, u'spotify track 0zmzyhaemvwq5crstru1fp', 223)\n",
      "\n",
      "(14, u'spotify track 5tve3pk05pyfigdsy9j4dj', 220)\n",
      "\n",
      "(15, u'spotify track 53qf56cjza9rtuumzdrsa6', 218)\n",
      "\n",
      "(16, u'spotify track 1vdz0vkfr5jnecmwiuamxk', 217)\n",
      "\n",
      "(17, u'spotify track 6lanrgr6wxibzr8kgzxxbl', 217)\n",
      "\n",
      "(18, u'spotify track 5ucax9htnlzgybistd3vdh', 216)\n",
      "\n",
      "(19, u'spotify track 5fvzc9gim4e8vu99w0xf6j', 212)\n",
      "\n",
      "(20, u'spotify track 7vrriwrloyvaoae3a9wjhe', 205)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print (i+1, context[i], track_dict[context[i]] )\n",
    "    print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cooccurence_matrix = np.zeros((len(vocabulary), len(context)))\n",
    "for k in range(len(vocabulary)):\n",
    "    for l in range(len(playlist_dict)):\n",
    "        indices = [i for i, x in enumerate(playlist_dict[l]) if x == vocabulary[k]]\n",
    "        for i in indices:  \n",
    "            for j in range(20): # Window size of 20\n",
    "                if i-j >= 0:\n",
    "                    if playlist_dict[l][i-j] in context:\n",
    "                        index = context.index(playlist_dict[l][i-j])\n",
    "                        cooccurence_matrix[k][index] += 1                \n",
    "                if i+j <= len(playlist_dict[l])-1:\n",
    "                    if playlist_dict[l][i+j] in context:\n",
    "                        index = context.index(playlist_dict[l][i+j])\n",
    "                        cooccurence_matrix[k][index] += 1\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09547739 0.00486809 0.00643844 ... 0.         0.00031407 0.        ]\n",
      " [0.00488497 0.09171131 0.0085093  ... 0.         0.00015758 0.        ]\n",
      " [0.00704346 0.00927676 0.09448548 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.04347826 0.04347826 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.seterr(invalid='ignore')\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "Probability_c_given_w = csr_matrix((len(vocabulary), len(context)))\n",
    "sum_of_rows = np.sum(cooccurence_matrix, axis=1)\n",
    "sum_of_rows = sum_of_rows.reshape((len(sum_of_rows),1))\n",
    "\n",
    "Probability_c_given_w = cooccurence_matrix/sum_of_rows\n",
    "Probability_c_given_w[np.isnan(Probability_c_given_w)] = 0\n",
    "\n",
    "print (Probability_c_given_w)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00437415 0.0042628  0.00401603 ... 0.00011185 0.00014896 0.00012639]\n"
     ]
    }
   ],
   "source": [
    "Overall_probability_distribution = csr_matrix(len(context))\n",
    "\n",
    "total_sum = cooccurence_matrix.sum()\n",
    "#for k in range(len(context)):\n",
    " #   sum_of_column = cooccurence_matrix[:, k].sum()\n",
    "  #  Overall_probability_distribution = sum_of_column/float(total_sum)\n",
    "sum_of_columns = np.sum(cooccurence_matrix, axis=0)\n",
    "Overall_probability_distribution = sum_of_columns / float(total_sum)\n",
    "\n",
    "print Overall_probability_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3390069  0.05766347 0.20498363 ... 0.         0.32394315 0.        ]\n",
      " [0.047968   1.33272775 0.32609652 ... 0.         0.02441614 0.        ]\n",
      " [0.20689263 0.33770099 1.37156792 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         1.008577   1.034475   ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Representation of each word in the vocabulary as a vector \n",
    "np.seterr(divide='ignore')\n",
    "from math import log\n",
    "vector_matrix = csr_matrix((len(vocabulary), len(context)))\n",
    "#vector_matrix = np.divide(Probability_c_given_w, Overall_probability_distribution[:, None], out=np.zeros_like(Probability_c_given_w), where=(Overall_probability_distribution)!=0)\n",
    "\n",
    "Overall_probability_distribution = Overall_probability_distribution.reshape(len(Overall_probability_distribution), 1)\n",
    "vector_matrix = np.divide(Probability_c_given_w, np.transpose(Overall_probability_distribution))\n",
    "vector_matrix = np.log10(vector_matrix)\n",
    "#vector_matrix = np.log10(vocabulary_vector_matrix)    \n",
    "zero_matrix = np.zeros((len(vocabulary), len(context)))\n",
    "vocabulary_vector_matrix = np.maximum(vector_matrix, zero_matrix)\n",
    "\n",
    "vocabulary_vector_matrix[np.isnan(vocabulary_vector_matrix)] = 0\n",
    "\n",
    "\n",
    "print(vocabulary_vector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering the words using KMeans algorithm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=100).fit(vocabulary_vector_matrix)\n",
    "\n",
    "clusters = {}\n",
    "s = 0\n",
    "for label in kmeans.labels_:\n",
    "    if label not in clusters:\n",
    "        clusters[label] = []\n",
    "    clusters[label].append(vocabulary[s])\n",
    "    s = s+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 25000)\n"
     ]
    }
   ],
   "source": [
    "# Finding the nearest neighbors of the top 20 most frequent words in the dictionary\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "distance_matrix = cosine_distances(vocabulary_vector_matrix[:15000], vocabulary_vector_matrix)\n",
    "print distance_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (distance_matrix.shape)\n",
    "\n",
    "songs_similarity = {}\n",
    "\n",
    "for i in range(len(distance_matrix)):\n",
    "    sorted_indices = np.argsort(distance_matrix[i])\n",
    "    \n",
    "    uri_track = vocabulary[i].split(' ')[2]\n",
    "    songs_similarity[uri_track] = []\n",
    "    \n",
    "    for j in range(len(sorted_indices)-1):\n",
    "        sorted_track = vocabulary[sorted_indices[j+1]].split(' ')[2]\n",
    "        songs_similarity[uri_track].append(sorted_track)                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print songs_similarity['7KXjTSCq5nL1LoYtL7XAwS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
